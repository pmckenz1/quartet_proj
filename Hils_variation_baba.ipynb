{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env ipython2\n",
    "\n",
    "\"\"\" D-statistic calculations \"\"\"\n",
    "# pylint: disable=E1101\n",
    "# pylint: disable=F0401\n",
    "# pylint: disable=W0142\n",
    "# pylint: disable=R0915\n",
    "# pylint: disable=R0914\n",
    "# pylint: disable=R0912\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "## ipyrad tools\n",
    "import toytree\n",
    "from ipyrad.assemble.write_outfiles import reftrick, GETCONS2\n",
    "from ipyrad.assemble.util import IPyradWarningExit, IPyradError, progressbar\n",
    "from ipyrad.analysis.bpp import Params\n",
    "from ipyrad.plotting.baba_panel_plot import baba_panel_plot\n",
    "\n",
    "#import scipy.stats as st  ## used for dfoil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba\n",
    "import itertools\n",
    "import datetime\n",
    "import types\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "\n",
    "## non-standard imports\n",
    "try: \n",
    "    import msprime as ms\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "## set floating point precision in data frames to 3 for prettier printing\n",
    "pd.set_option('precision', 3)\n",
    "\n",
    "\n",
    "class Baba(object):\n",
    "    \"new baba class object\"\n",
    "    def __init__(self, \n",
    "        data=None, \n",
    "        tests=None, \n",
    "        newick=None, \n",
    "        nboots=1000, \n",
    "        mincov=1):\n",
    "        \"\"\" \n",
    "        ipyrad.analysis Baba Class object.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : string or ndarray\n",
    "            A string path to a .loci file produced by ipyrad. Alternatively, \n",
    "            data can be entered as a Numpy array of float allele frequencies \n",
    "            with dimension (nloci, 4 or 5, maxlen). See simulation example \n",
    "            in the docs.\n",
    "            \n",
    "        tests : dict or list of dicts\n",
    "            A dictionary mapping Sample names to test taxon names, e.g., \n",
    "            test = {'p1': ['a', 'b'], 'p2': ['c'], 'p3': ['e'], 'p4': ['f']}.\n",
    "            Four taxon tests should have p1-p4 whereas five taxon tests will \n",
    "            used if dict keys are p1-p5. Other key names will raise an error. \n",
    "            The highest value name (e.g., p5) is the outgroup. \n",
    "        \n",
    "        newick: str\n",
    "            ...\n",
    "    \n",
    "        Functions\n",
    "        ---------\n",
    "        run()\n",
    "            ...\n",
    "        generate_tests_from_tree()\n",
    "            ...\n",
    "        plot()\n",
    "            ...\n",
    "        \"\"\"\n",
    "        ## parse data as (1) path to data file, or (2) ndarray\n",
    "        if isinstance(data, str):\n",
    "            self.data = os.path.realpath(data)\n",
    "        else:\n",
    "            self.data = data\n",
    "\n",
    "        if isinstance(newick, toytree.tree):\n",
    "            self.newick = newick.tree.write()\n",
    "        else:\n",
    "            self.newick = newick\n",
    "\n",
    "        ## store tests, check for errors\n",
    "        self.tests = tests\n",
    "\n",
    "        ## parameters\n",
    "        self.params = Params()\n",
    "        self.params.mincov = mincov\n",
    "        self.params.nboots = nboots\n",
    "        self.params.quiet = False\n",
    "        self.params.database = None\n",
    "\n",
    "        ## results storage\n",
    "        self.results_table = None\n",
    "        self.results_boots = None\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, \n",
    "        ipyclient=None,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Run a batch of dstat tests on a list of tests, where each test is \n",
    "        a dictionary mapping sample names to {p1 - p4} (and sometimes p5). \n",
    "        Parameters modifying the behavior of the run, such as the number\n",
    "        of bootstrap replicates (nboots) or the minimum coverage for \n",
    "        loci (mincov) can be set in {object}.params.\n",
    "        Parameters:\n",
    "        -----------\n",
    "        ipyclient (ipyparallel.Client object):\n",
    "            An ipyparallel client object to distribute jobs to a cluster. \n",
    "        \"\"\"\n",
    "        self.results_table, self.results_boots = batch(self, ipyclient)\n",
    "        self.results_table.nloci = np.nan_to_num(self.results_table.nloci)\\\n",
    "                                                 .astype(int)\n",
    "\n",
    "\n",
    "    def generate_tests_from_tree(self, \n",
    "        constraint_dict=None, \n",
    "        constraint_exact=True, \n",
    "        verbose=True):\n",
    "        \"\"\" \n",
    "        Returns a list of all possible 4-taxon tests on a tree (newick file). \n",
    "        The number of possible tests can be greatly reduced by setting \n",
    "        constraints on the taxon sampling using the constraint_dict arg. \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ... \n",
    "        \"\"\"\n",
    "        if not self.newick:\n",
    "            raise AttributeError(\"no newick tree information in {self}.newick\")\n",
    "        tests = tree2tests(self.newick, constraint_dict, constraint_exact)\n",
    "        if verbose:\n",
    "            print(\"{} tests generated from tree\".format(len(tests)))\n",
    "        self.tests = tests\n",
    "\n",
    "\n",
    "    def plot(self, \n",
    "        show_test_labels=True, \n",
    "        use_edge_lengths=False, \n",
    "        collapse_outgroup=False, \n",
    "        pct_tree_x=0.5, \n",
    "        pct_tree_y=0.7,\n",
    "        *args, \n",
    "        **kwargs):\n",
    "\n",
    "        \"\"\" draw a multi-panel figure with tree, tests, and results \"\"\"\n",
    "\n",
    "        ## check for attributes\n",
    "        if not self.newick:\n",
    "            raise IPyradError(\"baba plot requires a newick treefile\")\n",
    "        if not self.tests:\n",
    "            raise IPyradError(\"baba plot must have a .tests attribute\")\n",
    "\n",
    "        ## ensure tests is a list\n",
    "        if isinstance(self.tests, dict):\n",
    "            self.tests = [self.tests]\n",
    "\n",
    "        ## re-decompose the tree\n",
    "        ttree = toytree.tree(\n",
    "            self.newick, \n",
    "            orient='down', \n",
    "            use_edge_lengths=use_edge_lengths,\n",
    "            )\n",
    "\n",
    "        ## make the plot\n",
    "        canvas, axes, panel = baba_panel_plot(\n",
    "            ttree=ttree,\n",
    "            tests=self.tests,\n",
    "            boots=self.results_boots,\n",
    "            show_test_labels=show_test_labels, \n",
    "            use_edge_lengths=use_edge_lengths, \n",
    "            collapse_outgroup=collapse_outgroup, \n",
    "            pct_tree_x=pct_tree_x,\n",
    "            pct_tree_y=pct_tree_y,\n",
    "            *args, \n",
    "            **kwargs)\n",
    "        return canvas, axes, panel\n",
    "\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\" returns a copy of the baba analysis object \"\"\"\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "\n",
    "\n",
    "def batch(\n",
    "    baba,\n",
    "    ipyclient=None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    distributes jobs to the parallel client\n",
    "    \"\"\"\n",
    "\n",
    "    ## parse args\n",
    "    handle = baba.data\n",
    "    taxdicts = baba.tests\n",
    "    mindicts = baba.params.mincov\n",
    "    nboots = baba.params.nboots\n",
    "\n",
    "    ## if ms generator make into reusable list\n",
    "    sims = 0\n",
    "    if isinstance(handle, types.GeneratorType):\n",
    "        handle = list(handle)\n",
    "        sims = 1\n",
    "    else:\n",
    "        ## expand locifile path to full path\n",
    "        handle = os.path.realpath(handle)\n",
    "\n",
    "    ## parse taxdicts into names and lists if it a dictionary\n",
    "    #if isinstance(taxdicts, dict):\n",
    "    #    names, taxdicts = taxdicts.keys(), taxdicts.values()\n",
    "    #else:\n",
    "    #    names = []\n",
    "    names = []\n",
    "    if isinstance(taxdicts, dict):\n",
    "        taxdicts = [taxdicts]\n",
    "\n",
    "    ## an array to hold results (len(taxdicts), nboots)\n",
    "    tot = len(taxdicts)\n",
    "    resarr = np.zeros((tot, 7), dtype=np.float64)\n",
    "    bootsarr = np.zeros((tot, nboots), dtype=np.float64)\n",
    "    paneldict = {}\n",
    "\n",
    "    ## TODO: Setup a wrapper to find and cleanup ipyclient\n",
    "    ## define the function and parallelization to use, \n",
    "    ## if no ipyclient then drops back to using multiprocessing.\n",
    "    if not ipyclient:\n",
    "        # ipyclient = ip.core.parallel.get_client(**self._ipcluster)\n",
    "        raise IPyradError(\"you must enter an ipyparallel.Client() object\")\n",
    "    else:\n",
    "        lbview = ipyclient.load_balanced_view()\n",
    "\n",
    "    ## submit jobs to run on the cluster queue\n",
    "    start = time.time()\n",
    "    asyncs = {}\n",
    "    idx = 0\n",
    "\n",
    "    ## prepare data before sending to engines\n",
    "    ## if it's a str (locifile) then parse it\n",
    "    if isinstance(handle, str):\n",
    "        with open(handle, 'r') as infile:\n",
    "            loci = infile.read().strip().split(\"|\\n\")\n",
    "    if isinstance(handle, list):\n",
    "        pass #sims()\n",
    "\n",
    "    ## iterate over tests (repeats mindicts if fewer than taxdicts)\n",
    "    for test, mindict in zip(taxdicts, itertools.cycle([mindicts])):\n",
    "        ## if it's sim data then convert to an array\n",
    "        if sims:\n",
    "            arr = _msp_to_arr(handle, test)\n",
    "            args = (arr, test, mindict, nboots)\n",
    "            print(\"not yet implemented\")\n",
    "            #asyncs[idx] = lbview.apply_async(dstat, *args)\n",
    "        else:\n",
    "            args = [loci, test, mindict, nboots]\n",
    "            asyncs[idx] = lbview.apply(dstat, *args)\n",
    "        idx += 1\n",
    "\n",
    "    ## block until finished, print progress if requested.\n",
    "    try:\n",
    "        while 1:\n",
    "            keys = [i for (i, j) in asyncs.items() if j.ready()]\n",
    "            ## check for failures\n",
    "            for job in keys:\n",
    "                if not asyncs[job].successful():\n",
    "                    raise IPyradWarningExit(\\\n",
    "                        \" error: {}: {}\".format(job, asyncs[job].exception()))\n",
    "                ## enter results for successful jobs\n",
    "                else:\n",
    "                    _res, _bot = asyncs[job].result()\n",
    "                    ## store D4 results\n",
    "                    if _res.shape[0] == 1:\n",
    "                        resarr[job] = _res.T.as_matrix()[:, 0]\n",
    "                        bootsarr[job] = _bot\n",
    "                    ## store D5 results                        \n",
    "                    else:   \n",
    "                        paneldict[job] = _res\n",
    "\n",
    "\n",
    "\n",
    "                    del asyncs[job]\n",
    "\n",
    "            ## count finished\n",
    "            fin = tot - len(asyncs) \n",
    "            elap = datetime.timedelta(seconds=int(time.time()-start))\n",
    "            progressbar(tot, fin, \n",
    "                \" calculating D-stats  | {} | \".format(elap), spacer=\"\")\n",
    "            time.sleep(0.1)\n",
    "            if not asyncs:\n",
    "                print(\"\")#\\n\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt as inst:\n",
    "        ## cancel all jobs (ipy & multiproc modes) and then raise error\n",
    "        try:\n",
    "            ipyclient.abort()\n",
    "        except Exception:\n",
    "            pass\n",
    "        raise inst\n",
    "\n",
    "    ## dress up resarr as a Pandas DataFrame\n",
    "    if not names:\n",
    "        names = range(len(taxdicts))\n",
    "    #print(\"resarr\")\n",
    "    #print(resarr)\n",
    "    resarr = pd.DataFrame(resarr, \n",
    "        index=names,\n",
    "        columns=[\"dstat\", \"bootmean\", \"bootstd\", \"Z\", \"ABBA\", \"BABA\", \"nloci\"])\n",
    "\n",
    "    ## sort results and bootsarr to match if test names were supplied\n",
    "    resarr = resarr.sort_index()\n",
    "    order = [list(resarr.index).index(i) for i in names]\n",
    "    bootsarr = bootsarr[order]\n",
    "    return resarr, bootsarr\n",
    "\n",
    "\n",
    "\n",
    "def dstat(inarr, taxdict, mindict=1, nboots=1000, name=0):\n",
    "    \"\"\" private function to perform a single D-stat test\"\"\"\n",
    "\n",
    "    # ## get data as an array from loci file\n",
    "    # ## if loci-list then parse arr from loci\n",
    "    if isinstance(inarr, list):\n",
    "        arr, _ = _loci_to_arr(inarr, taxdict, mindict)\n",
    "    \n",
    "    # ## if it's an array already then go ahead\n",
    "    # elif isinstance(inarr, np.ndarray):\n",
    "    #     arr = inarr\n",
    "    # ## if it's a simulation object get freqs from array\n",
    "    # elif isinstance(inarr, Sim):\n",
    "    #     arr = _msp_to_arr(inarr, taxdict)\n",
    "\n",
    "    #elif isinstance(inarr, types.GeneratorType):\n",
    "    #    arr = _msp_to_arr(inarr, taxdict)\n",
    "    #elif isinstance(inarr, list):\n",
    "    #    arr = _msp_to_arr(inarr, taxdict)\n",
    "    ## get data from Sim object, do not digest the ms generator\n",
    "    #else:\n",
    "    #    raise Exception(\"Must enter either a 'locifile' or 'arr'\")\n",
    "\n",
    "    ## run tests\n",
    "    if len(taxdict) == 4:\n",
    "\n",
    "        ## get results\n",
    "        res, boots = _get_signif_4(arr, nboots)\n",
    "    \n",
    "        ## make res into a nice DataFrame\n",
    "        res = pd.DataFrame(res, \n",
    "            columns=[name],\n",
    "            index=[\"Dstat\", \"bootmean\", \"bootstd\", \"Z\", \"ABBA\", \"BABA\", \"nloci\"])\n",
    "\n",
    "    else:\n",
    "        ## get results\n",
    "        res, boots = _get_signif_5(arr, nboots)\n",
    "         ## make int a DataFrame\n",
    "        res = pd.DataFrame(res,\n",
    "            index=[\"p3\", \"p4\", \"shared\"], \n",
    "            columns=[\"Dstat\", \"bootmean\", \"bootstd\", \"Z\", \"ABxxA\", \"BAxxA\", \"nloci\"]\n",
    "            )\n",
    "\n",
    "    return res.T, boots\n",
    "\n",
    "\n",
    "\n",
    "def _loci_to_arr(loci, taxdict, mindict):\n",
    "    \"\"\"\n",
    "    return a frequency array from a loci file for all loci with taxa from \n",
    "    taxdict and min coverage from mindict. \n",
    "    \"\"\"\n",
    "\n",
    "    ## make the array (4 or 5) and a mask array to remove loci without cov\n",
    "    nloci = len(loci)\n",
    "    keep = np.zeros(nloci, dtype=np.bool_)\n",
    "    arr = np.zeros((nloci, 4, 300), dtype=np.float64)\n",
    "    if len(taxdict) == 5:\n",
    "        arr = np.zeros((nloci, 6, 300), dtype=np.float64)\n",
    "\n",
    "    ## if not mindict, make one that requires 1 in each taxon\n",
    "    if isinstance(mindict, int):\n",
    "        mindict = {i: mindict for i in taxdict}\n",
    "    elif isinstance(mindict, dict):\n",
    "        mindict = {i: mindict[i] for i in taxdict}\n",
    "    else:\n",
    "        mindict = {i: 1 for i in taxdict}\n",
    "\n",
    "    ## raise error if names are not 'p[int]' \n",
    "    allowed_names = ['p1', 'p2', 'p3', 'p4', 'p5']\n",
    "    if any([i not in allowed_names for i in taxdict]):\n",
    "        raise IPyradError(\\\n",
    "            \"keys in taxdict must be named 'p1' through 'p4' or 'p5'\")\n",
    "\n",
    "    ## parse key names\n",
    "    keys = sorted([i for i in taxdict.keys() if i[0] == 'p'])\n",
    "    outg = keys[-1]\n",
    "\n",
    "    ## grab seqs just for the good guys\n",
    "    for loc in xrange(nloci):\n",
    "\n",
    "        ## parse the locus\n",
    "        lines = loci[loc].split(\"\\n\")[:-1]\n",
    "        names = [i.split()[0] for i in lines]\n",
    "        seqs = np.array([list(i.split()[1]) for i in lines])\n",
    "\n",
    "        ## check that names cover the taxdict (still need to check by site)\n",
    "        covs = [sum([j in names for j in taxdict[tax]]) >= mindict[tax] \\\n",
    "                for tax in taxdict]\n",
    "\n",
    "        ## keep locus\n",
    "        if all(covs):\n",
    "            keep[loc] = True\n",
    "\n",
    "            ## get the refseq\n",
    "            refidx = np.where([i in taxdict[outg] for i in names])[0]\n",
    "            refseq = seqs[refidx].view(np.uint8)\n",
    "            ancestral = np.array([reftrick(refseq, GETCONS2)[:, 0]])\n",
    "\n",
    "            ## freq of ref in outgroup\n",
    "            iseq = _reffreq2(ancestral, refseq, GETCONS2)\n",
    "            arr[loc, -1, :iseq.shape[1]] = iseq \n",
    "\n",
    "            ## enter 4-taxon freqs\n",
    "            if len(taxdict) == 4:\n",
    "                for tidx, key in enumerate(keys[:-1]):\n",
    "\n",
    "                    ## get idx of names in test tax\n",
    "                    nidx = np.where([i in taxdict[key] for i in names])[0]\n",
    "                    sidx = seqs[nidx].view(np.uint8)\n",
    "                   \n",
    "                    ## get freq of sidx\n",
    "                    iseq = _reffreq2(ancestral, sidx, GETCONS2)\n",
    "                   \n",
    "                    ## fill it in \n",
    "                    arr[loc, tidx, :iseq.shape[1]] = iseq\n",
    "\n",
    "            else:\n",
    "\n",
    "                ## entere p5; and fill it in\n",
    "                iseq = _reffreq2(ancestral, refseq, GETCONS2) \n",
    "                arr[loc, -1, :iseq.shape[1]] = iseq \n",
    "                \n",
    "                ## enter p1\n",
    "                nidx = np.where([i in taxdict['p1'] for i in names])[0]\n",
    "                sidx = seqs[nidx].view(np.uint8)\n",
    "                iseq = _reffreq2(ancestral, sidx, GETCONS2)\n",
    "                arr[loc, 0, :iseq.shape[1]] = iseq\n",
    "                \n",
    "                ## enter p2\n",
    "                nidx = np.where([i in taxdict['p2'] for i in names])[0]\n",
    "                sidx = seqs[nidx].view(np.uint8)\n",
    "                iseq = _reffreq2(ancestral, sidx, GETCONS2)\n",
    "                arr[loc, 1, :iseq.shape[1]] = iseq\n",
    "                \n",
    "                ## enter p3 with p4 masked, and p4 with p3 masked\n",
    "                nidx = np.where([i in taxdict['p3'] for i in names])[0]\n",
    "                nidy = np.where([i in taxdict['p4'] for i in names])[0]\n",
    "                sidx = seqs[nidx].view(np.uint8)\n",
    "                sidy = seqs[nidy].view(np.uint8)\n",
    "                xseq = _reffreq2(ancestral, sidx, GETCONS2)\n",
    "                yseq = _reffreq2(ancestral, sidy, GETCONS2)\n",
    "                mask3 = xseq != 0\n",
    "                mask4 = yseq != 0\n",
    "                xseq[mask4] = 0\n",
    "                yseq[mask3] = 0\n",
    "                arr[loc, 2, :xseq.shape[1]] = xseq\n",
    "                arr[loc, 3, :yseq.shape[1]] = yseq\n",
    "                \n",
    "                ## enter p34 \n",
    "                nidx = nidx.tolist() + nidy.tolist()\n",
    "                sidx = seqs[nidx].view(np.uint8)\n",
    "                iseq = _reffreq2(ancestral, sidx, GETCONS2)\n",
    "                arr[loc, 4, :iseq.shape[1]] = iseq\n",
    "\n",
    "\n",
    "    ## size-down array to the number of loci that have taxa for the test\n",
    "    arr = arr[keep, :, :]\n",
    "\n",
    "    ## size-down sites to \n",
    "    arr = masknulls(arr)\n",
    "\n",
    "    return arr, keep\n",
    "\n",
    "\n",
    "\n",
    "def tree2tests(newick, constraint_dict=None, constraint_exact=False):\n",
    "    \"\"\"\n",
    "    Returns dict of all possible four-taxon splits in a tree. Assumes\n",
    "    the user has entered a rooted tree. Skips polytomies.\n",
    "    \"\"\"\n",
    "    ## make tree\n",
    "    tree = toytree.tree(newick)\n",
    "    testset = set()\n",
    "    \n",
    "    ## constraints\n",
    "    cdict = {\"p1\":[], \"p2\":[], \"p3\":[], \"p4\":[]}\n",
    "    if constraint_dict:\n",
    "        cdict.update(constraint_dict)\n",
    "\n",
    "    ## traverse root to tips. Treat the left as outgroup, then the right.\n",
    "    tests = []\n",
    "    \n",
    "    ## topnode must have children\n",
    "    for topnode in tree.tree.traverse(\"levelorder\"):\n",
    "        for oparent in topnode.children:\n",
    "            for onode in oparent.traverse(\"levelorder\"):\n",
    "                if test_constraint(onode, cdict, \"p4\", constraint_exact):\n",
    "                    #print(topnode.name, onode.name)\n",
    "                    \n",
    "                    ## p123 parent is sister to oparent\n",
    "                    p123parent = oparent.get_sisters()[0]\n",
    "                    for p123node in p123parent.traverse(\"levelorder\"):\n",
    "                        for p3parent in p123node.children:\n",
    "                            for p3node in p3parent.traverse(\"levelorder\"):\n",
    "                                if test_constraint(p3node, cdict, \"p3\", constraint_exact):\n",
    "                                    #print(topnode.name, onode.name, p3node.name)\n",
    "                                    \n",
    "                                    ## p12 parent is sister to p3parent\n",
    "                                    p12parent = p3parent.get_sisters()[0]\n",
    "                                    for p12node in p12parent.traverse(\"levelorder\"):\n",
    "                                        for p2parent in p12node.children:\n",
    "                                            for p2node in p2parent.traverse(\"levelorder\"):\n",
    "                                                if test_constraint(p2node, cdict, \"p2\", constraint_exact):\n",
    "\n",
    "                                                    ## p12 parent is sister to p3parent\n",
    "                                                    p1parent = p2parent.get_sisters()[0]\n",
    "                                                    for p1node in p1parent.traverse(\"levelorder\"):\n",
    "                                                        for p1parent in p1node.children:\n",
    "                                                            for p1node in p1parent.traverse(\"levelorder\"):\n",
    "                                                                if test_constraint(p1node, cdict, \"p1\", constraint_exact):\n",
    "                                                                    x = (onode.name, p3node.name, p2node.name, p1node.name)\n",
    "                                                                    test = {}\n",
    "                                                                    test['p4'] = onode.get_leaf_names()\n",
    "                                                                    test['p3'] = p3node.get_leaf_names()\n",
    "                                                                    test['p2'] = p2node.get_leaf_names()\n",
    "                                                                    test['p1'] = p1node.get_leaf_names()\n",
    "                                                                    if x not in testset:\n",
    "                                                                        tests.append(test)\n",
    "                                                                        testset.add(x)\n",
    "    return tests                                            \n",
    "\n",
    "\n",
    "\n",
    "# def tree2tests(newick, constraint_dict=None, constraint_exact=True):\n",
    "#     \"\"\"\n",
    "#     Returns dict of all possible four-taxon splits in a tree. Assumes\n",
    "#     the user has entered a rooted tree. Skips polytomies.\n",
    "#     \"\"\"\n",
    "#     ## make tree\n",
    "#     tree = toytree.ete3mini.Tree(newick)\n",
    "#     testset = set()\n",
    "    \n",
    "#     ## constraints\n",
    "#     cdict = {\"p1\":[], \"p2\":[], \"p3\":[], \"p4\":[]}\n",
    "#     if constraint_dict:\n",
    "#         cdict.update(constraint_dict)\n",
    "\n",
    "#     ## traverse root to tips. Treat the left as outgroup, then the right.\n",
    "#     tests = []\n",
    "#     ## topnode must have children\n",
    "#     for topnode in tree.traverse(\"levelorder\"):\n",
    "#         for oparent in topnode.children:\n",
    "#             for onode in oparent.traverse(\"levelorder\"):\n",
    "#                 if test_constraint(onode, cdict, \"p4\", constraint_exact):\n",
    "#                     #print(topnode.name, onode.name)\n",
    "                    \n",
    "#                     ## p123 parent is sister to oparent\n",
    "#                     p123parent = oparent.get_sisters()[0]\n",
    "#                     for p123node in p123parent.traverse(\"levelorder\"):\n",
    "#                         for p3parent in p123node.children:\n",
    "#                             for p3node in p3parent.traverse(\"levelorder\"):\n",
    "#                                 if test_constraint(p3node, cdict, \"p3\", constraint_exact):\n",
    "#                                     #print(topnode.name, onode.name, p3node.name)\n",
    "                                    \n",
    "#                                     ## p12 parent is sister to p3 parent\n",
    "#                                     p12parent = p3parent.get_sisters()[0]\n",
    "#                                     for p12node in p12parent.traverse(\"levelorder\"):\n",
    "#                                         if p12node.children:\n",
    "#                                             p2parent = p12node.children[1]#for p2parent in p12parent.children[1]:\n",
    "#                                             p1parent = p12node.children[0]\n",
    "#                                             for p2node in p2parent.traverse(\"levelorder\"):\n",
    "#                                                 if test_constraint(p2node, cdict, \"p2\", constraint_exact):\n",
    "#                                                     for p1node in p1parent.traverse(\"levelorder\"):\n",
    "#                                                         if test_constraint(p1node, cdict, \"p1\", constraint_exact):\n",
    "#                                                             test = {}\n",
    "#                                                             test['p4'] = onode.get_leaf_names()\n",
    "#                                                             test['p3'] = p3node.get_leaf_names()\n",
    "#                                                             test['p2'] = p2node.get_leaf_names()\n",
    "#                                                             test['p1'] = p1node.get_leaf_names()\n",
    "#                                                             x = list(itertools.chain(*[sorted(test[\"p4\"]) + \\\n",
    "#                                                                                        sorted(test[\"p3\"]) + \\\n",
    "#                                                                                        sorted(test[\"p2\"]) + \\\n",
    "#                                                                                        sorted(test[\"p1\"])]))\n",
    "#                                                             x = \"_\".join(x)\n",
    "#                                                             if x not in testset:\n",
    "#                                                                 tests.append(test)\n",
    "#                                                                 testset.add(x)\n",
    "#         return tests\n",
    "\n",
    "\n",
    "\n",
    "def test_constraint(node, cdict, tip, exact):\n",
    "    names = set(node.get_leaf_names())\n",
    "    const = set(cdict[tip])\n",
    "    if const:\n",
    "        if exact:\n",
    "            #if len(names.intersection(const)) == len(const):\n",
    "            if names == const:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            if len(names.intersection(const)) == len(names):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0        \n",
    "    return 1\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def masknulls(arr):\n",
    "    nvarr = np.zeros(arr.shape[0], dtype=np.int8)\n",
    "    trimarr = np.zeros(arr.shape, dtype=np.float64)\n",
    "    for loc in xrange(arr.shape[0]):\n",
    "        nvars = 0\n",
    "        for site in xrange(arr.shape[2]):\n",
    "            col = arr[loc, :, site]\n",
    "            ## mask cols with 9s\n",
    "            if not np.any(col == 9):\n",
    "                ## any non-outgroup shows variation?\n",
    "                ## todo: check whether BBBBA is ever info?\n",
    "                if np.any(col[:-1] != col[0]):\n",
    "                    trimarr[loc, :, nvars] = col\n",
    "                    nvars += 1\n",
    "        nvarr[loc] = nvars        \n",
    "    return trimarr[:, :, :nvarr.max()]\n",
    "\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def _reffreq2(ancestral, iseq, consdict):\n",
    "    ## empty arrays\n",
    "    freq = np.zeros((1, iseq.shape[1]), dtype=np.float64)\n",
    "    amseq = np.zeros((iseq.shape[0]*2, iseq.shape[1]), dtype=np.uint8)\n",
    "    \n",
    "    ## fill in both copies\n",
    "    for seq in xrange(iseq.shape[0]):\n",
    "        for col in xrange(iseq.shape[1]):  \n",
    "\n",
    "            ## get this base and check if it is hetero\n",
    "            base = iseq[seq][col]\n",
    "            who = consdict[:, 0] == base\n",
    "            \n",
    "            ## if not hetero then enter it\n",
    "            if not np.any(who):\n",
    "                amseq[seq*2][col] = base\n",
    "                amseq[seq*2+1][col] = base        \n",
    "            ## if hetero then enter the 2 resolutions\n",
    "            else:\n",
    "                amseq[seq*2][col] = consdict[who, 1][0]\n",
    "                amseq[seq*2+1][col] = consdict[who, 2][0]\n",
    "\n",
    "    ## amseq may have N or -, these need to be masked\n",
    "    for i in xrange(amseq.shape[1]):\n",
    "        ## without N or -\n",
    "        reduced = amseq[:, i][amseq[:, i] != 9]\n",
    "        counts = reduced != ancestral[0][i]\n",
    "        if reduced.shape[0]:\n",
    "            freq[:, i] = counts.sum() / reduced.shape[0]\n",
    "        else:\n",
    "            freq[:, i] = 9\n",
    "    return freq\n",
    "\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def _prop_dstat(arr):\n",
    "    \n",
    "    ## numerator\n",
    "    abba = ((1.-arr[:, 0]) * (arr[:, 1]) * (arr[:, 2]) * (1.-arr[:, 3]))  \n",
    "    baba = ((arr[:, 0]) * (1.-arr[:, 1]) * (arr[:, 2]) * (1.-arr[:, 3]))\n",
    "    top = abba - baba\n",
    "    bot = abba + baba\n",
    "\n",
    "    ## get statistic and avoid zero div  \n",
    "    sbot = bot.sum()\n",
    "    if  sbot != 0:\n",
    "        dst = top.sum() / float(sbot)\n",
    "    else:\n",
    "        dst = 0\n",
    "    \n",
    "    return abba.sum(), baba.sum(), dst\n",
    "\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def _get_boots(arr, nboots):\n",
    "    \"\"\"\n",
    "    return array of bootstrap D-stats\n",
    "    \"\"\"\n",
    "    ## hold results (nboots, [dstat, ])\n",
    "    boots = np.zeros((nboots,))\n",
    "    \n",
    "    ## iterate to fill boots\n",
    "    for bidx in xrange(nboots):\n",
    "        ## sample with replacement\n",
    "        lidx = np.random.randint(0, arr.shape[0], arr.shape[0])\n",
    "        tarr = arr[lidx]\n",
    "        _, _, dst = _prop_dstat(tarr)\n",
    "        boots[bidx] = dst\n",
    "    \n",
    "    ## return bootarr\n",
    "    return boots\n",
    "\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def _get_signif_4(arr, nboots):\n",
    "    \"\"\"\n",
    "    returns a list of stats and an array of dstat boots. Stats includes\n",
    "    z-score and two-sided P-value. \n",
    "    \"\"\"\n",
    "    abba, baba, dst = _prop_dstat(arr)\n",
    "    boots = _get_boots(arr, nboots)\n",
    "    estimate, stddev = (boots.mean(), boots.std())\n",
    "    zscore = 0.\n",
    "    if stddev:\n",
    "        zscore = np.abs(dst) / stddev\n",
    "    stats = [dst, estimate, stddev, zscore, abba, baba, arr.shape[0]]\n",
    "    return np.array(stats), boots\n",
    "\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def _get_signif_5(arr, nboots):\n",
    "    \"\"\"\n",
    "    returns a list of stats and an array of dstat boots. Stats includes\n",
    "    z-score and two-sided P-value. \n",
    "    \"\"\"\n",
    "\n",
    "    statsarr = np.zeros((3, 7), dtype=np.float64)\n",
    "    bootsarr = np.zeros((3, nboots))\n",
    "\n",
    "    idx = 0\n",
    "    for acol in [2, 3, 4]:\n",
    "        rows = np.array([0, 1, acol, 5])\n",
    "        tarr = arr[:, rows, :]\n",
    "\n",
    "        abxa, baxa, dst = _prop_dstat(tarr)\n",
    "        boots = _get_boots(tarr, nboots)\n",
    "        estimate, stddev = (boots.mean(), boots.std())\n",
    "        if stddev:\n",
    "            zscore = np.abs(dst) / stddev\n",
    "        else:\n",
    "            zscore = np.NaN\n",
    "        stats = [dst, estimate, stddev, zscore, abxa, baxa, arr.shape[0]]\n",
    "\n",
    "        statsarr[idx] = stats\n",
    "        bootsarr[idx] = boots\n",
    "        idx += 1\n",
    "\n",
    "    return statsarr, bootsarr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "## Simulation functions (requires msprime)\n",
    "######################################################################\n",
    "\n",
    "\n",
    "class Sim(object):\n",
    "    def __init__(self, names, sims, nreps, debug):\n",
    "        self.names = names\n",
    "        self.sims = sims\n",
    "        self.nreps = nreps\n",
    "        self.debug = debug\n",
    "\n",
    "\n",
    "def _simulate(self, nreps, admix=None, Ns=500000, gen=20):\n",
    "    \"\"\"\n",
    "    Enter a baba.Tree object in which the 'tree' attribute (newick \n",
    "    derived tree) has edge lengths in units of generations. You can \n",
    "    use the 'gen' parameter to multiply branch lengths by a constant. \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nreps: (int)\n",
    "        Number of reps (loci) to simulate under the demographic scenario\n",
    "    tree: (baba.Tree object)\n",
    "        A baba.Tree object initialized by calling baba.Tree(*args). \n",
    "    admix: (list)\n",
    "        A list of admixture events to occur on the tree. Nodes must be \n",
    "        reference by their index number, and events must occur in time\n",
    "        intervals when edges exist. Use the .draw() function of the \n",
    "        baba.Tree object to see node index numbers and coalescent times.\n",
    "    Ns: (float)\n",
    "        Fixed effective population size for all lineages (may allow to vary\n",
    "        in the future). \n",
    "    gen: (int)\n",
    "        A multiplier applied to branch lengths to scale into units of \n",
    "        generations. Example, if all edges on a tree were 1 then you might\n",
    "        enter 50000 to multiply so that edges are 50K generations long.\n",
    "    \"\"\"\n",
    "\n",
    "    ## node ages\n",
    "    Taus = np.array(list(set(self.verts[:, 1]))) * 1e4 * gen\n",
    "\n",
    "    ## The tips samples, ordered alphanumerically\n",
    "    ## Population IDs correspond to their indexes in pop config\n",
    "    ntips = len(self.tree)\n",
    "    #names = {name: idx for idx, name in enumerate(sorted(self.tree.get_leaf_names()))}\n",
    "    ## rev ladderized leaf name order (left to right on downward facing tree)\n",
    "    names = {name: idx for idx, name in enumerate(self.tree.get_leaf_names()[::-1])}\n",
    "    pop_config = [\n",
    "        ms.PopulationConfiguration(sample_size=2, initial_size=Ns)\n",
    "        for i in range(ntips)\n",
    "    ]\n",
    "\n",
    "    ## migration matrix all zeros init\n",
    "    migmat = np.zeros((ntips, ntips)).tolist()\n",
    "\n",
    "    ## a list for storing demographic events\n",
    "    demog = []\n",
    "\n",
    "    ## coalescent times\n",
    "    coals = sorted(list(set(self.verts[:, 1])))[1:]\n",
    "    for ct in xrange(len(coals)):\n",
    "        ## check for admix event before next coalescence\n",
    "        ## ...\n",
    "        \n",
    "        ## print coals[ct], nidxs, time\n",
    "        nidxs = np.where(self.verts[:, 1] == coals[ct])[0]\n",
    "        time = Taus[ct+1]\n",
    "\n",
    "        ## add coalescence at each node\n",
    "        for nidx in nidxs:\n",
    "            node = self.tree.search_nodes(name=str(nidx))[0]\n",
    "\n",
    "            ## get destionation (lowest child idx number), and other\n",
    "            dest = sorted(node.get_leaves(), key=lambda x: x.idx)[0]\n",
    "            otherchild = [i for i in node.children if not \n",
    "                          i.get_leaves_by_name(dest.name)][0]\n",
    "\n",
    "            ## get source\n",
    "            if otherchild.is_leaf():\n",
    "                source = otherchild\n",
    "            else:\n",
    "                source = sorted(otherchild.get_leaves(), key=lambda x: x.idx)[0]\n",
    "            \n",
    "            ## add coal events\n",
    "            event = ms.MassMigration(\n",
    "                        time=int(time),\n",
    "                        source=names[source.name], \n",
    "                        destination=names[dest.name], \n",
    "                        proportion=1.0)\n",
    "            #print(int(time), names[source.name], names[dest.name])\n",
    "        \n",
    "            ## ...\n",
    "            demog.append(event)\n",
    "            \n",
    "            \n",
    "    ## sim the data\n",
    "    replicates = ms.simulate(\n",
    "        population_configurations=pop_config,\n",
    "        migration_matrix=migmat,\n",
    "        demographic_events=demog,\n",
    "        num_replicates=nreps,\n",
    "        length=100, \n",
    "        mutation_rate=1e-8)\n",
    "    return replicates\n",
    "\n",
    "\n",
    "\n",
    "## simulates data on 12 taxon tree with two admixture events\n",
    "def _sim_admix_12(nreps, Ns=500000, gen=20):\n",
    "    \n",
    "    # Set the ML values of various parameters\n",
    "    Taus = np.array([0, 1, 2, 3, 4, 5]) * 1e4 * gen\n",
    "\n",
    "    # Migration rates C -> B and from IJ -> EF\n",
    "    m_C_B = 2e-6\n",
    "    m_IJ_EF = 2e-6\n",
    "    \n",
    "    # Population IDs correspond to their indexes in pop_config.\n",
    "    ntips = len(tree.tree)\n",
    "    pop_config = [\n",
    "        ms.PopulationConfiguration(sample_size=2, initial_size=Ns)\n",
    "        for i in range(ntips)]\n",
    "    \n",
    "    ## migration matrix all zeros time=0\n",
    "    migmat = np.zeros((ntips, ntips)).tolist()\n",
    "    \n",
    "    ## set up demography\n",
    "    demog = [\n",
    "        ## initial migration from C -> B\n",
    "        ms.MigrationRateChange(time=0, rate=m_C_B, matrix_index=(1, 2)),\n",
    "        ms.MigrationRateChange(time=Taus[1], rate=0),\n",
    "\n",
    "        # merge events at time 1 (b,a), (f,e), (j,i)\n",
    "        ms.MassMigration(time=Taus[1], source=1, destination=0, proportion=1.0), \n",
    "        ms.MassMigration(time=Taus[1], source=5, destination=4, proportion=1.0), \n",
    "        ms.MassMigration(time=Taus[1], source=9, destination=8, proportion=1.0), \n",
    "        \n",
    "        ## migration from IJ -> EF (backward in time)\n",
    "        ms.MigrationRateChange(time=Taus[1], rate=m_IJ_EF, matrix_index=(4, 8)), \n",
    "\n",
    "        ## merge events at time 2 (c,a), (g,e), (k,i)\n",
    "        ms.MassMigration(time=Taus[2], source=2, destination=0, proportion=1.0), \n",
    "        ms.MassMigration(time=Taus[2], source=6, destination=4, proportion=1.0), \n",
    "        ms.MassMigration(time=Taus[2], source=10, destination=8, proportion=1.0), \n",
    "\n",
    "        ## end migration at ABC and merge\n",
    "        ms.MigrationRateChange(time=Taus[2], rate=0),\n",
    "        ms.MassMigration(time=Taus[3], source=3, destination=0, proportion=1.0), \n",
    "        ms.MassMigration(time=Taus[3], source=7, destination=4, proportion=1.0), \n",
    "        ms.MassMigration(time=Taus[3], source=11, destination=8, proportion=1.0),   \n",
    "        \n",
    "        ## merge EFJH -> IJKL\n",
    "        ms.MassMigration(time=Taus[4], source=8, destination=4, proportion=1.0),   \n",
    "        \n",
    "        ## merge ABCD -> EFJHIJKL\n",
    "        ms.MassMigration(time=Taus[5], source=4, destination=0, proportion=1.0),   \n",
    "    ]\n",
    "\n",
    "    ## sim the data\n",
    "    replicates = ms.simulate(\n",
    "        population_configurations=pop_config,\n",
    "        migration_matrix=migmat,\n",
    "        demographic_events=demog,\n",
    "        num_replicates=nreps,\n",
    "        length=100, \n",
    "        mutation_rate=1e-9)\n",
    "    \n",
    "    return replicates\n",
    "\n",
    "\n",
    "\n",
    "#def _msp_to_arr(simreps, test):\n",
    "def _msp_to_arr(Sim, test):\n",
    "    \n",
    "    ## the fixed tree dictionary\n",
    "    #fix = {j: [i, i+1] for j, i in zip(list(\"abcdefghijkl\"), range(0, 24, 2))}\n",
    "    fix = {j: [i, i+1] for j, i in zip(Sim.names, range(0, len(Sim.names)*2, 2))}\n",
    "    \n",
    "    ## fill taxdict by test\n",
    "    keys = ['p1', 'p2', 'p3', 'p4']\n",
    "    arr = np.zeros((Sim.nreps, 4, 100))\n",
    "    \n",
    "    ## unless it's a 5-taxon test\n",
    "    if len(test) == 5:\n",
    "        arr = np.zeros((100000, 6, 100))\n",
    "        keys += ['p5']\n",
    "    \n",
    "    ## create array sampler for taxa\n",
    "    taxs = [test[key] for key in keys]\n",
    "    idxs = [list(itertools.chain(*[fix[j] for j in i])) for i in taxs]\n",
    "\n",
    "    ## iterate over reps filling arr\n",
    "    idx = 0\n",
    "    for trees in simreps:\n",
    "        \n",
    "        ## build genotype array\n",
    "        shape = trees.get_num_mutations(), trees.get_sample_size()\n",
    "        garr = np.empty(shape, dtype=\"u1\")\n",
    "    \n",
    "        ## fill the garr\n",
    "        for variant in trees.variants():\n",
    "            garr[variant.index] = variant.genotypes\n",
    "        \n",
    "        if len(test) == 4:\n",
    "            if garr.shape[0]:\n",
    "                ## fill my arr with freqs\n",
    "                for pdx, tax in enumerate(idxs):\n",
    "                    freq = garr[:, tax]\n",
    "                    freq = freq.sum(axis=1) / float(freq.shape[1])\n",
    "                    maxsz = min(freq.shape[0], 100)\n",
    "                    arr[idx, pdx, :maxsz] = freq[:maxsz]\n",
    "        else:\n",
    "            if garr.shape[0]:\n",
    "                ## get the easy ones\n",
    "                p1 = garr[:, idxs[0]]\n",
    "                p2 = garr[:, idxs[1]]\n",
    "                p5 = garr[:, idxs[4]]\n",
    "                p34 = garr[:, idxs[2]+idxs[3]]\n",
    "\n",
    "                ## identity of SNPs is important\n",
    "                p3 = garr[:, idxs[2]]\n",
    "                p4 = garr[:, idxs[3]]\n",
    "                \n",
    "                ## any rows with data in b are masked in a\n",
    "                mask3 = np.where(p3.sum(axis=1) == 0)[0]\n",
    "                mask4 = np.where(p4.sum(axis=1) == 0)[0]\n",
    "                masked_p3 = p3[mask4]\n",
    "                masked_p4 = p4[mask3]\n",
    "                \n",
    "                ## enter frequencies\n",
    "                freq = p1\n",
    "                freq = freq.sum(axis=1) / float(freq.shape[1])\n",
    "                maxsz = min(freq.shape[0], 100)\n",
    "                arr[idx, 0, :maxsz] = freq[:maxsz]\n",
    "                \n",
    "                freq = p2\n",
    "                freq = freq.sum(axis=1) / float(freq.shape[1])\n",
    "                maxsz = min(freq.shape[0], 100)\n",
    "                arr[idx, 1, :maxsz] = freq[:maxsz]\n",
    "               \n",
    "                freq = masked_p3\n",
    "                freq = freq.sum(axis=1) / float(freq.shape[1])\n",
    "                maxsz = min(freq.shape[0], 100)\n",
    "                arr[idx, 2, :maxsz] = freq[:maxsz]               \n",
    "               \n",
    "                freq = masked_p4\n",
    "                freq = freq.sum(axis=1) / float(freq.shape[1])\n",
    "                maxsz = min(freq.shape[0], 100)\n",
    "                arr[idx, 3, :maxsz] = freq[:maxsz]\n",
    "               \n",
    "                freq = p34\n",
    "                freq = freq.sum(axis=1) / float(freq.shape[1])\n",
    "                maxsz = min(freq.shape[0], 100)\n",
    "                arr[idx, 4, :maxsz] = freq[:maxsz]\n",
    "                 \n",
    "                freq = p5\n",
    "                freq = freq.sum(axis=1) / float(freq.shape[1])\n",
    "                maxsz = min(freq.shape[0], 100)\n",
    "                arr[idx, 5, :maxsz] = freq[:maxsz]\n",
    "        idx += 1\n",
    "\n",
    "    ## reduce the size of arr to min loci        \n",
    "    arr = arr[:idx+1]\n",
    "    \n",
    "    ## reduce the size of arr to min len\n",
    "    minl = np.where(np.all(np.all(arr==0, axis=1) == True, axis=0))[0]\n",
    "    if np.any(minl):\n",
    "        minl = minl.min()\n",
    "    else:\n",
    "        minl = None\n",
    "    arr = arr[:, :, :minl]\n",
    "    \n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "## combines sim_admix12 + msp_to_arr + baba to return single (stats, boots)\n",
    "def sim_admix_12_baba(nreps, test, mindict, nboots):\n",
    "    sims = _sim_admix_12(nreps)\n",
    "    arr = _msp_to_arr(sims, test)\n",
    "    stats, boots = baba(arr, test, mindict, nboots)\n",
    "    return stats, boots\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ## test input files\n",
    "    LOCIFILE = \"/home/deren/Dropbox/RADexplore/EmpVib/\" \\\n",
    "              +\"vib_half_64tip_c85d6m4p99.loci\"\n",
    "\n",
    "    # ## taxon list to parse from LOCIFILE\n",
    "    TAXONLIST = ['acutifolium_DRY3_MEX_006',\n",
    "                 'sulcatum_D9_MEX_003',\n",
    "                 'jamesonii_D12_PWS_1636',\n",
    "                 'triphyllum_D13_PWS_1783',\n",
    "                 'dentatum_ELS4']\n",
    "\n",
    "    ## calculate dstats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
